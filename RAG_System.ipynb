{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "def load_pdf_and_split(path):\n",
        "    pdf_loader = PyPDFLoader(path)\n",
        "    pdf_pages = pdf_loader.load_and_split()\n",
        "    text_chunks = pdf_pages[0].page_content\n",
        "    return text_chunks\n",
        "\n",
        "def split_text_into_chunks(text_chunks, chunk_size):\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size)\n",
        "    chunks = text_splitter.create_documents([text_chunks])\n",
        "    return chunks\n",
        "\n",
        "def embed_documents(text_chunks, api_key, model):\n",
        "    embedding_model = GoogleGenerativeAIEmbeddings(google_api_key=api_key, model=model)\n",
        "    vectors = embedding_model.embed_documents(text_chunks)\n",
        "    return vectors\n",
        "\n",
        "def create_vector_store(chunks, embedding_model):\n",
        "    db = Chroma.from_documents(chunks, embedding_model)\n",
        "    db.persist()\n",
        "    return db\n",
        "\n",
        "def create_retriever(db_connection):\n",
        "    retriever = db_connection.as_retriever(search_kwargs={\"k\": 5})\n",
        "    return retriever\n",
        "\n",
        "def build_rag_chain(retriever, chat_template, api_key, model):\n",
        "    output_parser = StrOutputParser()\n",
        "    rag_chain = (\n",
        "        {\"context\": retriever | (lambda docs: \"\\n\\n\".join(doc.page_content for doc in docs)),\n",
        "         \"question\": RunnablePassthrough()}\n",
        "        | chat_template\n",
        "        | (ChatGoogleGenerativeAI(google_api_key=api_key, model=model)\n",
        "          | output_parser)\n",
        "    )\n",
        "    return rag_chain\n",
        "\n",
        "def main():\n",
        "    path = '/content/RAG System.pdf'\n",
        "    text_chunks = load_pdf_and_split(path)\n",
        "    chunks = split_text_into_chunks(text_chunks, chunk_size=300)\n",
        "    with open(\"/content/api.txt\") as f:\n",
        "        api_key = f.read().strip()\n",
        "    vectors = embed_documents(text_chunks, api_key, model=\"models/embedding-001\")\n",
        "    db = create_vector_store(chunks, GoogleGenerativeAIEmbeddings(google_api_key=api_key, model=\"models/embedding-001\"))\n",
        "    db_connection = Chroma(embedding_function=GoogleGenerativeAIEmbeddings(google_api_key=api_key, model=\"models/embedding-001\"))\n",
        "    retriever = create_retriever(db_connection)\n",
        "    chat_template = ChatPromptTemplate.from_messages([\n",
        "        SystemMessage(content=\"I'm a helpful AI assistant. I'll use the provided document to answer your questions.\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"\"\"Answer the following question based on the provided context:\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        Answer:\"\"\")\n",
        "    ])\n",
        "    model = \"gemini-1.5-pro-latest\"\n",
        "    rag_chain = build_rag_chain(retriever, chat_template, api_key, model)\n",
        "    user_question = input(\"Enter your question: \")\n",
        "    logging.info(f\"User question: {user_question}\")\n",
        "    response = rag_chain.invoke(user_question)\n",
        "    print(response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im5UOrCesNBP",
        "outputId": "a819fcce-4ae3-4a9f-f3de-85af7817b9d3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question: Can you explain the main contribution of the Leave No Context Behind paper?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Leave No Context Behind: Main Contribution\n",
            "\n",
            "The main contribution of the \"Leave No Context Behind\" paper is the introduction of **Infini-attention**, a novel attention mechanism designed to efficiently scale Transformer-based Large Language Models (LLMs) to handle infinitely long input sequences while maintaining bounded memory and computation requirements. \n",
            "\n",
            "Here's a breakdown of how Infini-attention achieves this:\n",
            "\n",
            "* **Combines Local and Long-Term Attention:** Infini-attention integrates both masked local attention (focusing on recent context) and long-term linear attention (accessing information from the distant past) within a single Transformer block. This allows the model to capture both immediate and historical context effectively.\n",
            "* **Compressive Memory:** The key innovation lies in incorporating a compressive memory system into the attention mechanism. Unlike traditional attention, which has memory requirements that grow quadratically with sequence length, compressive memory uses a fixed number of parameters to store and recall information. This enables efficient processing of extremely long sequences without exorbitant memory costs. \n",
            "* **Bounded Memory and Computation:** By utilizing compressive memory and a combination of local and long-term attention, Infini-attention ensures that both memory usage and computational cost remain bounded even as the input sequence length increases. This makes it feasible to apply LLMs to tasks involving massive amounts of data, such as long-form document summarization or analyzing extensive codebases.\n",
            "* **Fast Streaming Inference:** The design of Infini-attention enables fast streaming inference for LLMs. This means the model can process input data as a continuous stream, making it suitable for real-time applications and tasks with ongoing data input.\n",
            "\n",
            "Overall, the \"Leave No Context Behind\" paper presents a significant advancement in LLM architecture by proposing Infini-attention, which effectively addresses the challenges of handling long sequences and paving the way for more efficient and scalable language models. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9ZOrGsmEsMsB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}